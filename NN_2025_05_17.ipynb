{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFx7ZCuMcmiUzEaWczt0Zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogdanpetrovsky/PINN/blob/main/NN_2025_05_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Завантаження CSV\n",
        "df = pd.read_csv(\"Results.csv\")\n",
        "df.columns = df.columns.str.strip()"
      ],
      "metadata": {
        "id": "MzrD7EozoHNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "INI6XgcLhgFc",
        "outputId": "5a7214a8-c71a-43c5-97ad-8fc8c6a34fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.107963\n",
            "Epoch 100, Loss: 0.005057\n",
            "Epoch 200, Loss: 0.004891\n",
            "Epoch 300, Loss: 0.004671\n",
            "Epoch 400, Loss: 0.004300\n",
            "Epoch 500, Loss: 0.003751\n",
            "Epoch 600, Loss: 0.003412\n",
            "Epoch 700, Loss: 0.003354\n",
            "Epoch 800, Loss: 0.003219\n",
            "Epoch 900, Loss: 0.003167\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Real: [2.4882002e-02 2.0500520e+08 2.8390000e-02], Predicted: [ 2.3335652e-02  2.0133784e+08 -1.0666081e+02]\n",
            "🔍 Real: [2.2697000e-02 1.9824469e+08 2.4323000e-02], Predicted: [2.2796778e-02 2.0110373e+08 1.9730696e+02]\n",
            "🔍 Real: [5.0808001e-02 2.1040566e+08 3.6366001e-02], Predicted: [5.0856054e-02 2.0947766e+08 1.5728624e+03]\n",
            "🔍 Real: [2.0435000e-02 1.9653614e+08 2.0761000e-02], Predicted: [ 2.0228613e-02  1.9743730e+08 -6.1803841e+01]\n",
            "🔍 Real: [4.4427000e-02 1.9855547e+08 2.4419000e-02], Predicted: [4.4574432e-02 1.9949706e+08 1.2669900e+03]\n"
          ]
        }
      ],
      "source": [
        "# Вхідні та вихідні ознаки\n",
        "X = df[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']]\n",
        "Y = df[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']]\n",
        "\n",
        "# Масштабування\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_Y = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "Y_scaled = scaler_Y.fit_transform(Y)\n",
        "\n",
        "# Розділення\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Перетворення в тензори\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Нейронна мережа (глибока)\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(3, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Модель, оптимізатор, функція втрат\n",
        "model = DeepNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Тренування\n",
        "losses = []\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "# Побудова графіку втрат\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Оцінка на тесті\n",
        "model.eval()\n",
        "pred = model(X_test).detach().numpy()\n",
        "true = y_test.numpy()\n",
        "\n",
        "# Зворотне масштабування\n",
        "pred_orig = scaler_Y.inverse_transform(pred)\n",
        "true_orig = scaler_Y.inverse_transform(true)\n",
        "\n",
        "# Порівняння\n",
        "for i in range(5):\n",
        "    print(f\"🔍 Real: {true_orig[i]}, Predicted: {pred_orig[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Results.csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "X = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values\n",
        "# Вихід (реакція системи)\n",
        "y = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp', 'End_angle', 'Residual_angle']].values\n",
        "\n",
        "# Масштабування\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    outputs = tf.keras.layers.Dense(5)(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3. Physics-informed loss\n",
        "# Наприклад, формулюємо просту втрату на основі закону Гука як перевірку:\n",
        "# σ = E * ε → перевіримо чи виконується ця пропорція між Young's modulus, MaxPlasticStrain і MaxVonMises\n",
        "\n",
        "def physics_informed_loss(y_true, y_pred):\n",
        "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "    # Відновлюємо фізичні параметри (розмасштабовані)\n",
        "    y_pred_phys = scaler_y.inverse_transform(y_pred)\n",
        "    strain = y_pred_phys[:, 0]  # MaxPlasticStrain\n",
        "    stress = y_pred_phys[:, 1]  # MaxVonMises\n",
        "    disp = y_pred_phys[:, 2]    # MaxDisp\n",
        "\n",
        "    # Відновимо E з X\n",
        "    X_inv = scaler_X.inverse_transform(model.input)\n",
        "    E_modulus = X_inv[:, 2]\n",
        "\n",
        "    # Гук: σ ≈ E * ε  → перевірка, наскільки виконується ця умова\n",
        "    stress_pred = E_modulus * strain\n",
        "    physics_loss = tf.reduce_mean(tf.square(stress - stress_pred))\n",
        "\n",
        "    return mse_loss + 0.1 * physics_loss  # вагу можна тюнити\n",
        "\n",
        "# 4. Компіляція та тренування\n",
        "model.compile(optimizer='adam', loss=physics_informed_loss)\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32)\n",
        "\n",
        "# Збереження\n",
        "model.save(\"pinn_deformation_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ko55fu7mqXHq",
        "outputId": "8ac20614-5d8d-48c9-8dbb-5de28cdd512f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Cannot convert a symbolic tf.Tensor (functional_1/dense_3_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-56a44c3a8a36>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# 4. Компіляція та тренування\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphysics_informed_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Збереження\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-56a44c3a8a36>\u001b[0m in \u001b[0;36mphysics_informed_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Відновлюємо фізичні параметри (розмасштабовані)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0my_pred_phys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mstrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_phys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# MaxPlasticStrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mstress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_phys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# MaxVonMises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n\u001b[1;32m   1058\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic tf.Tensor (functional_1/dense_3_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Завантаження даних\n",
        "data = pd.read_csv(\"Results.csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Вхідні параметри\n",
        "X = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values.astype(np.float32)\n",
        "# Вихід\n",
        "y = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']].values.astype(np.float32)\n",
        "\n",
        "# Масштабування\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X).astype(np.float32)\n",
        "y_scaled = scaler_y.fit_transform(y).astype(np.float32)\n",
        "\n",
        "# Тренувальний розподіл\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    outputs = tf.keras.layers.Dense(3)(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# 3. Кастомний train step з physics loss\n",
        "@tf.function\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x_batch, training=True)\n",
        "        mse_loss = tf.reduce_mean(tf.square(y_batch - y_pred))\n",
        "\n",
        "        # Inverse scaling для y\n",
        "        y_pred_phys = y_pred * tf.constant(scaler_y.scale_, dtype=tf.float32) + tf.constant(scaler_y.mean_, dtype=tf.float32)\n",
        "        strain = y_pred_phys[:, 0]  # MaxPlasticStrain\n",
        "        stress = y_pred_phys[:, 1]  # MaxVonMises\n",
        "\n",
        "        # Inverse scaling для X\n",
        "        x_inv = x_batch * tf.constant(scaler_X.scale_, dtype=tf.float32) + tf.constant(scaler_X.mean_, dtype=tf.float32)\n",
        "        E_modulus = x_inv[:, 2]\n",
        "\n",
        "        # Кліп значення E, щоб уникнути переповнення\n",
        "        E_modulus = tf.clip_by_value(E_modulus, 1e6, 3e11)\n",
        "\n",
        "        # Physics loss: σ ≈ E·ε\n",
        "        stress_pred = E_modulus * strain\n",
        "        physics_loss = tf.reduce_mean(tf.square(stress - stress_pred))\n",
        "\n",
        "        total_loss = mse_loss + 0.1 * physics_loss\n",
        "        total_loss = tf.where(tf.math.is_nan(total_loss), tf.constant(0.0, dtype=tf.float32), total_loss)\n",
        "\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return total_loss\n",
        "\n",
        "# 4. Навчання\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        epoch_loss += loss\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {epoch_loss.numpy():.4f}\")\n",
        "\n",
        "# 5. Збереження моделі\n",
        "model.save(\"pinn_deformation_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Ytk4cjraOb",
        "outputId": "93f54060-ed4e-424e-bd54-6741b0c44f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 142694799441395712.0000\n",
            "Epoch 50, Loss: 51393122304.0000\n",
            "Epoch 100, Loss: 30355322880.0000\n",
            "Epoch 150, Loss: 17281243136.0000\n",
            "Epoch 200, Loss: 908106.2500\n",
            "Epoch 250, Loss: 19514705920.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Завантаження даних\n",
        "data = pd.read_csv(\"Results (1).csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Прибираємо рядки з пропущеними значеннями\n",
        "data = data.dropna()\n",
        "\n",
        "# Вхідні параметри\n",
        "X_raw = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values\n",
        "y_raw = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']].values  # без кутів\n",
        "\n",
        "# Масштабування\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_raw)\n",
        "y_scaled = scaler_y.fit_transform(y_raw)\n",
        "\n",
        "# Розділення на train/val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.Dense(64, activation='tanh')(inputs)\n",
        "    x = tf.keras.layers.Dense(64, activation='tanh')(x)\n",
        "    outputs = tf.keras.layers.Dense(3)(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3. Стандартне переднавчання (без physics)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse')\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=300, batch_size=32)\n",
        "\n",
        "# 4. Оголошення фізично-інформованого навчання\n",
        "lambda_phy = 0.01  # коефіцієнт фізичних втрат\n",
        "\n",
        "# Перетворення на float32 (TensorFlow очікує саме float32)\n",
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "# Перехід на низькорівневе навчання з physics loss\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "batch_size = 32\n",
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.permutation(len(X_train))\n",
        "    X_train = X_train[idx]\n",
        "    y_train = y_train[idx]\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = X_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            mse_loss = tf.reduce_mean(tf.square(y_batch - y_pred))\n",
        "\n",
        "            # Розмасштабуємо\n",
        "            y_pred_phys = scaler_y.inverse_transform(y_pred.numpy())\n",
        "            strain = y_pred_phys[:, 0]  # MaxPlasticStrain\n",
        "            stress = y_pred_phys[:, 1]  # MaxVonMises\n",
        "\n",
        "            # Відновлюємо Young's modulus\n",
        "            X_inv = scaler_X.inverse_transform(X_batch.numpy())\n",
        "            E = X_inv[:, 2]\n",
        "\n",
        "            # Нормалізація перед physics loss\n",
        "            strain_norm = (strain - np.mean(strain)) / (np.std(strain) + 1e-8)\n",
        "            stress_norm = (stress - np.mean(stress)) / (np.std(stress) + 1e-8)\n",
        "            E_norm = (E - np.mean(E)) / (np.std(E) + 1e-8)\n",
        "\n",
        "            stress_pred = E_norm * strain_norm\n",
        "            physics_loss = np.mean((stress_norm - stress_pred) ** 2)\n",
        "\n",
        "            total_loss = mse_loss + lambda_phy * physics_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse_loss.numpy():.4f}, Physics: {physics_loss:.4f}, Total: {total_loss.numpy():.4f}\")\n",
        "\n",
        "# Збереження моделі\n",
        "model.save(\"pinn_deformation_model_v2.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2xN6ZeJW9DPp",
        "outputId": "5b6d58a4-dbaa-4ee5-b22e-0fb0b18ba48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.6964 - val_loss: 0.2685\n",
            "Epoch 2/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.5168 - val_loss: 0.2226\n",
            "Epoch 3/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.5354 - val_loss: 0.2202\n",
            "Epoch 4/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.5256 - val_loss: 0.2193\n",
            "Epoch 5/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.2812 - val_loss: 0.2199\n",
            "Epoch 6/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.3235 - val_loss: 0.2191\n",
            "Epoch 7/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.2835 - val_loss: 0.2187\n",
            "Epoch 8/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.3468 - val_loss: 0.2179\n",
            "Epoch 9/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2633 - val_loss: 0.2173\n",
            "Epoch 10/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.5093 - val_loss: 0.2162\n",
            "Epoch 11/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6321 - val_loss: 0.2155\n",
            "Epoch 12/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.2179 - val_loss: 0.2146\n",
            "Epoch 13/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.5058 - val_loss: 0.2145\n",
            "Epoch 14/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2283 - val_loss: 0.2134\n",
            "Epoch 15/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6195 - val_loss: 0.2132\n",
            "Epoch 16/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2736 - val_loss: 0.2143\n",
            "Epoch 17/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7622 - val_loss: 0.2119\n",
            "Epoch 18/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3408 - val_loss: 0.2124\n",
            "Epoch 19/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3175 - val_loss: 0.2130\n",
            "Epoch 20/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8580 - val_loss: 0.2105\n",
            "Epoch 21/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5606 - val_loss: 0.2098\n",
            "Epoch 22/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8045 - val_loss: 0.2080\n",
            "Epoch 23/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4510 - val_loss: 0.2075\n",
            "Epoch 24/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2725 - val_loss: 0.2089\n",
            "Epoch 25/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6805 - val_loss: 0.2071\n",
            "Epoch 26/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5232 - val_loss: 0.2052\n",
            "Epoch 27/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4915 - val_loss: 0.2047\n",
            "Epoch 28/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8660 - val_loss: 0.2038\n",
            "Epoch 29/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6506 - val_loss: 0.2028\n",
            "Epoch 30/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6136 - val_loss: 0.2012\n",
            "Epoch 31/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8911 - val_loss: 0.2002\n",
            "Epoch 32/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2143 - val_loss: 0.1994\n",
            "Epoch 33/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3538 - val_loss: 0.1995\n",
            "Epoch 34/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4490 - val_loss: 0.1975\n",
            "Epoch 35/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7652 - val_loss: 0.1967\n",
            "Epoch 36/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4075 - val_loss: 0.1952\n",
            "Epoch 37/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2646 - val_loss: 0.1956\n",
            "Epoch 38/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0219 - val_loss: 0.1921\n",
            "Epoch 39/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7417 - val_loss: 0.1911\n",
            "Epoch 40/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5242 - val_loss: 0.1894\n",
            "Epoch 41/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3634 - val_loss: 0.1893\n",
            "Epoch 42/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2682 - val_loss: 0.1876\n",
            "Epoch 43/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3522 - val_loss: 0.1867\n",
            "Epoch 44/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6771 - val_loss: 0.1848\n",
            "Epoch 45/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3045 - val_loss: 0.1834\n",
            "Epoch 46/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2153 - val_loss: 0.1832\n",
            "Epoch 47/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8416 - val_loss: 0.1821\n",
            "Epoch 48/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3192 - val_loss: 0.1811\n",
            "Epoch 49/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5589 - val_loss: 0.1791\n",
            "Epoch 50/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0859 - val_loss: 0.1782\n",
            "Epoch 51/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5908 - val_loss: 0.1770\n",
            "Epoch 52/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1928 - val_loss: 0.1770\n",
            "Epoch 53/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4222 - val_loss: 0.1766\n",
            "Epoch 54/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3376 - val_loss: 0.1757\n",
            "Epoch 55/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7337 - val_loss: 0.1739\n",
            "Epoch 56/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5876 - val_loss: 0.1735\n",
            "Epoch 57/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2271 - val_loss: 0.1745\n",
            "Epoch 58/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4321 - val_loss: 0.1726\n",
            "Epoch 59/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2881 - val_loss: 0.1728\n",
            "Epoch 60/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2020 - val_loss: 0.1722\n",
            "Epoch 61/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3562 - val_loss: 0.1710\n",
            "Epoch 62/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1819 - val_loss: 0.1698\n",
            "Epoch 63/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7904 - val_loss: 0.1701\n",
            "Epoch 64/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2234 - val_loss: 0.1703\n",
            "Epoch 65/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2333 - val_loss: 0.1706\n",
            "Epoch 66/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2911 - val_loss: 0.1705\n",
            "Epoch 67/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4343 - val_loss: 0.1680\n",
            "Epoch 68/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2456 - val_loss: 0.1697\n",
            "Epoch 69/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2771 - val_loss: 0.1693\n",
            "Epoch 70/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3348 - val_loss: 0.1680\n",
            "Epoch 71/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2638 - val_loss: 0.1678\n",
            "Epoch 72/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2733 - val_loss: 0.1674\n",
            "Epoch 73/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2003 - val_loss: 0.1665\n",
            "Epoch 74/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2436 - val_loss: 0.1684\n",
            "Epoch 75/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0103 - val_loss: 0.1660\n",
            "Epoch 76/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4553 - val_loss: 0.1654\n",
            "Epoch 77/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8992 - val_loss: 0.1645\n",
            "Epoch 78/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2037 - val_loss: 0.1657\n",
            "Epoch 79/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4317 - val_loss: 0.1649\n",
            "Epoch 80/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2893 - val_loss: 0.1653\n",
            "Epoch 81/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3579 - val_loss: 0.1644\n",
            "Epoch 82/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2175 - val_loss: 0.1638\n",
            "Epoch 83/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4600 - val_loss: 0.1629\n",
            "Epoch 84/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3468 - val_loss: 0.1638\n",
            "Epoch 85/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4341 - val_loss: 0.1621\n",
            "Epoch 86/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2016 - val_loss: 0.1616\n",
            "Epoch 87/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4932 - val_loss: 0.1625\n",
            "Epoch 88/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6100 - val_loss: 0.1608\n",
            "Epoch 89/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0634 - val_loss: 0.1607\n",
            "Epoch 90/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4554 - val_loss: 0.1607\n",
            "Epoch 91/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3366 - val_loss: 0.1621\n",
            "Epoch 92/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4494 - val_loss: 0.1608\n",
            "Epoch 93/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2039 - val_loss: 0.1617\n",
            "Epoch 94/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2935 - val_loss: 0.1618\n",
            "Epoch 95/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3096 - val_loss: 0.1607\n",
            "Epoch 96/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1983 - val_loss: 0.1603\n",
            "Epoch 97/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6429 - val_loss: 0.1600\n",
            "Epoch 98/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4365 - val_loss: 0.1602\n",
            "Epoch 99/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2009 - val_loss: 0.1601\n",
            "Epoch 100/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3938 - val_loss: 0.1587\n",
            "Epoch 101/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6267 - val_loss: 0.1582\n",
            "Epoch 102/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2465 - val_loss: 0.1604\n",
            "Epoch 103/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9153 - val_loss: 0.1577\n",
            "Epoch 104/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1913 - val_loss: 0.1580\n",
            "Epoch 105/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3606 - val_loss: 0.1592\n",
            "Epoch 106/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2440 - val_loss: 0.1594\n",
            "Epoch 107/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4850 - val_loss: 0.1576\n",
            "Epoch 108/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3658 - val_loss: 0.1597\n",
            "Epoch 109/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1496 - val_loss: 0.1577\n",
            "Epoch 110/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1871 - val_loss: 0.1581\n",
            "Epoch 111/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3232 - val_loss: 0.1580\n",
            "Epoch 112/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2955 - val_loss: 0.1587\n",
            "Epoch 113/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2354 - val_loss: 0.1591\n",
            "Epoch 114/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5174 - val_loss: 0.1563\n",
            "Epoch 115/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7842 - val_loss: 0.1558\n",
            "Epoch 116/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2699 - val_loss: 0.1585\n",
            "Epoch 117/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2865 - val_loss: 0.1580\n",
            "Epoch 118/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2325 - val_loss: 0.1581\n",
            "Epoch 119/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2803 - val_loss: 0.1552\n",
            "Epoch 120/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1763 - val_loss: 0.1552\n",
            "Epoch 121/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6886 - val_loss: 0.1563\n",
            "Epoch 122/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5538 - val_loss: 0.1555\n",
            "Epoch 123/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2045 - val_loss: 0.1572\n",
            "Epoch 124/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8748 - val_loss: 0.1555\n",
            "Epoch 125/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6110 - val_loss: 0.1552\n",
            "Epoch 126/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8021 - val_loss: 0.1545\n",
            "Epoch 127/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7426 - val_loss: 0.1553\n",
            "Epoch 128/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5069 - val_loss: 0.1543\n",
            "Epoch 129/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5125 - val_loss: 0.1536\n",
            "Epoch 130/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6480 - val_loss: 0.1552\n",
            "Epoch 131/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4492 - val_loss: 0.1549\n",
            "Epoch 132/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2352 - val_loss: 0.1556\n",
            "Epoch 133/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5985 - val_loss: 0.1551\n",
            "Epoch 134/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2073 - val_loss: 0.1551\n",
            "Epoch 135/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2985 - val_loss: 0.1569\n",
            "Epoch 136/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5224 - val_loss: 0.1547\n",
            "Epoch 137/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9159 - val_loss: 0.1532\n",
            "Epoch 138/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2030 - val_loss: 0.1550\n",
            "Epoch 139/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1632 - val_loss: 0.1534\n",
            "Epoch 140/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1350 - val_loss: 0.1534\n",
            "Epoch 141/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4350 - val_loss: 0.1541\n",
            "Epoch 142/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.8454 - val_loss: 0.1533\n",
            "Epoch 143/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3354 - val_loss: 0.1527\n",
            "Epoch 144/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2389 - val_loss: 0.1550\n",
            "Epoch 145/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4078 - val_loss: 0.1545\n",
            "Epoch 146/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7778 - val_loss: 0.1534\n",
            "Epoch 147/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5575 - val_loss: 0.1524\n",
            "Epoch 148/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.9043 - val_loss: 0.1528\n",
            "Epoch 149/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2914 - val_loss: 0.1536\n",
            "Epoch 150/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9512 - val_loss: 0.1538\n",
            "Epoch 151/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2945 - val_loss: 0.1530\n",
            "Epoch 152/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3063 - val_loss: 0.1542\n",
            "Epoch 153/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3380 - val_loss: 0.1534\n",
            "Epoch 154/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2277 - val_loss: 0.1537\n",
            "Epoch 155/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4453 - val_loss: 0.1519\n",
            "Epoch 156/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5383 - val_loss: 0.1527\n",
            "Epoch 157/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6973 - val_loss: 0.1530\n",
            "Epoch 158/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5277 - val_loss: 0.1518\n",
            "Epoch 159/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4654 - val_loss: 0.1511\n",
            "Epoch 160/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2381 - val_loss: 0.1542\n",
            "Epoch 161/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5883 - val_loss: 0.1517\n",
            "Epoch 162/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2108 - val_loss: 0.1536\n",
            "Epoch 163/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4848 - val_loss: 0.1527\n",
            "Epoch 164/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6668 - val_loss: 0.1519\n",
            "Epoch 165/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3172 - val_loss: 0.1516\n",
            "Epoch 166/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2669 - val_loss: 0.1530\n",
            "Epoch 167/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2889 - val_loss: 0.1519\n",
            "Epoch 168/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3764 - val_loss: 0.1517\n",
            "Epoch 169/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3328 - val_loss: 0.1538\n",
            "Epoch 170/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8221 - val_loss: 0.1503\n",
            "Epoch 171/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3323 - val_loss: 0.1514\n",
            "Epoch 172/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4222 - val_loss: 0.1512\n",
            "Epoch 173/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4439 - val_loss: 0.1500\n",
            "Epoch 174/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3285 - val_loss: 0.1519\n",
            "Epoch 175/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2606 - val_loss: 0.1523\n",
            "Epoch 176/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2520 - val_loss: 0.1517\n",
            "Epoch 177/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1677 - val_loss: 0.1504\n",
            "Epoch 178/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5054 - val_loss: 0.1503\n",
            "Epoch 179/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.0710 - val_loss: 0.1496\n",
            "Epoch 180/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4267 - val_loss: 0.1503\n",
            "Epoch 181/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3863 - val_loss: 0.1505\n",
            "Epoch 182/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3769 - val_loss: 0.1502\n",
            "Epoch 183/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0550 - val_loss: 0.1506\n",
            "Epoch 184/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3661 - val_loss: 0.1498\n",
            "Epoch 185/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5152 - val_loss: 0.1510\n",
            "Epoch 186/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4346 - val_loss: 0.1502\n",
            "Epoch 187/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5000 - val_loss: 0.1498\n",
            "Epoch 188/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3535 - val_loss: 0.1507\n",
            "Epoch 189/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4629 - val_loss: 0.1501\n",
            "Epoch 190/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4395 - val_loss: 0.1501\n",
            "Epoch 191/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9142 - val_loss: 0.1491\n",
            "Epoch 192/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5580 - val_loss: 0.1493\n",
            "Epoch 193/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5710 - val_loss: 0.1491\n",
            "Epoch 194/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3737 - val_loss: 0.1494\n",
            "Epoch 195/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2410 - val_loss: 0.1511\n",
            "Epoch 196/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4981 - val_loss: 0.1495\n",
            "Epoch 197/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.9571 - val_loss: 0.1488\n",
            "Epoch 198/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.9102 - val_loss: 0.1500\n",
            "Epoch 199/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2085 - val_loss: 0.1507\n",
            "Epoch 200/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2522 - val_loss: 0.1505\n",
            "Epoch 201/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8054 - val_loss: 0.1493\n",
            "Epoch 202/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3545 - val_loss: 0.1499\n",
            "Epoch 203/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5651 - val_loss: 0.1495\n",
            "Epoch 204/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3771 - val_loss: 0.1486\n",
            "Epoch 205/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0399 - val_loss: 0.1482\n",
            "Epoch 206/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3527 - val_loss: 0.1497\n",
            "Epoch 207/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4578 - val_loss: 0.1494\n",
            "Epoch 208/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2676 - val_loss: 0.1499\n",
            "Epoch 209/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4904 - val_loss: 0.1483\n",
            "Epoch 210/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7358 - val_loss: 0.1488\n",
            "Epoch 211/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1839 - val_loss: 0.1497\n",
            "Epoch 212/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2631 - val_loss: 0.1509\n",
            "Epoch 213/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2418 - val_loss: 0.1501\n",
            "Epoch 214/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.4949 - val_loss: 0.1481\n",
            "Epoch 215/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.7843 - val_loss: 0.1473\n",
            "Epoch 216/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1694 - val_loss: 0.1484\n",
            "Epoch 217/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.0274 - val_loss: 0.1473\n",
            "Epoch 218/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5773 - val_loss: 0.1484\n",
            "Epoch 219/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4130 - val_loss: 0.1478\n",
            "Epoch 220/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5734 - val_loss: 0.1477\n",
            "Epoch 221/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3541 - val_loss: 0.1483\n",
            "Epoch 222/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2901 - val_loss: 0.1499\n",
            "Epoch 223/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3303 - val_loss: 0.1483\n",
            "Epoch 224/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3452 - val_loss: 0.1483\n",
            "Epoch 225/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0467 - val_loss: 0.1473\n",
            "Epoch 226/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7532 - val_loss: 0.1474\n",
            "Epoch 227/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0845 - val_loss: 0.1469\n",
            "Epoch 228/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5151 - val_loss: 0.1475\n",
            "Epoch 229/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5104 - val_loss: 0.1482\n",
            "Epoch 230/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2713 - val_loss: 0.1481\n",
            "Epoch 231/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8254 - val_loss: 0.1478\n",
            "Epoch 232/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2403 - val_loss: 0.1482\n",
            "Epoch 233/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2395 - val_loss: 0.1494\n",
            "Epoch 234/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4068 - val_loss: 0.1485\n",
            "Epoch 235/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1741 - val_loss: 0.1478\n",
            "Epoch 236/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6569 - val_loss: 0.1468\n",
            "Epoch 237/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4345 - val_loss: 0.1469\n",
            "Epoch 238/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8433 - val_loss: 0.1469\n",
            "Epoch 239/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4356 - val_loss: 0.1477\n",
            "Epoch 240/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7383 - val_loss: 0.1466\n",
            "Epoch 241/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3951 - val_loss: 0.1469\n",
            "Epoch 242/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4345 - val_loss: 0.1474\n",
            "Epoch 243/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1934 - val_loss: 0.1475\n",
            "Epoch 244/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1844 - val_loss: 0.1486\n",
            "Epoch 245/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5686 - val_loss: 0.1467\n",
            "Epoch 246/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7115 - val_loss: 0.1464\n",
            "Epoch 247/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7296 - val_loss: 0.1470\n",
            "Epoch 248/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4508 - val_loss: 0.1473\n",
            "Epoch 249/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9534 - val_loss: 0.1468\n",
            "Epoch 250/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3283 - val_loss: 0.1474\n",
            "Epoch 251/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7831 - val_loss: 0.1455\n",
            "Epoch 252/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0681 - val_loss: 0.1461\n",
            "Epoch 253/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4525 - val_loss: 0.1458\n",
            "Epoch 254/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4222 - val_loss: 0.1453\n",
            "Epoch 255/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2780 - val_loss: 0.1466\n",
            "Epoch 256/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8700 - val_loss: 0.1458\n",
            "Epoch 257/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2897 - val_loss: 0.1479\n",
            "Epoch 258/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4017 - val_loss: 0.1458\n",
            "Epoch 259/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4235 - val_loss: 0.1456\n",
            "Epoch 260/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2042 - val_loss: 0.1476\n",
            "Epoch 261/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8990 - val_loss: 0.1458\n",
            "Epoch 262/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4331 - val_loss: 0.1458\n",
            "Epoch 263/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2746 - val_loss: 0.1471\n",
            "Epoch 264/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8936 - val_loss: 0.1450\n",
            "Epoch 265/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5433 - val_loss: 0.1457\n",
            "Epoch 266/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3231 - val_loss: 0.1460\n",
            "Epoch 267/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7675 - val_loss: 0.1449\n",
            "Epoch 268/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3876 - val_loss: 0.1448\n",
            "Epoch 269/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7745 - val_loss: 0.1448\n",
            "Epoch 270/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2735 - val_loss: 0.1472\n",
            "Epoch 271/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2372 - val_loss: 0.1467\n",
            "Epoch 272/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4753 - val_loss: 0.1444\n",
            "Epoch 273/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4705 - val_loss: 0.1450\n",
            "Epoch 274/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3937 - val_loss: 0.1450\n",
            "Epoch 275/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6533 - val_loss: 0.1448\n",
            "Epoch 276/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4587 - val_loss: 0.1443\n",
            "Epoch 277/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4395 - val_loss: 0.1445\n",
            "Epoch 278/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3492 - val_loss: 0.1445\n",
            "Epoch 279/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6854 - val_loss: 0.1446\n",
            "Epoch 280/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4185 - val_loss: 0.1443\n",
            "Epoch 281/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9348 - val_loss: 0.1434\n",
            "Epoch 282/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5713 - val_loss: 0.1439\n",
            "Epoch 283/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2167 - val_loss: 0.1457\n",
            "Epoch 284/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2577 - val_loss: 0.1460\n",
            "Epoch 285/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2784 - val_loss: 0.1444\n",
            "Epoch 286/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2647 - val_loss: 0.1446\n",
            "Epoch 287/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5433 - val_loss: 0.1435\n",
            "Epoch 288/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6772 - val_loss: 0.1437\n",
            "Epoch 289/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5182 - val_loss: 0.1435\n",
            "Epoch 290/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7277 - val_loss: 0.1426\n",
            "Epoch 291/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1195 - val_loss: 0.1431\n",
            "Epoch 292/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2791 - val_loss: 0.1441\n",
            "Epoch 293/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2703 - val_loss: 0.1446\n",
            "Epoch 294/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2564 - val_loss: 0.1447\n",
            "Epoch 295/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6592 - val_loss: 0.1422\n",
            "Epoch 296/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2590 - val_loss: 0.1436\n",
            "Epoch 297/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4988 - val_loss: 0.1421\n",
            "Epoch 298/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4679 - val_loss: 0.1417\n",
            "Epoch 299/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.7057 - val_loss: 0.1427\n",
            "Epoch 300/300\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3583 - val_loss: 0.1413\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4b3fdb4f2523>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Відновлюємо Young's modulus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mX_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_inv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Завантаження та підготовка даних\n",
        "data = pd.read_csv(\"Results (1).csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "data = data.dropna()\n",
        "\n",
        "X_raw = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values.astype(np.float32)\n",
        "y_raw = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']].values.astype(np.float32)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_raw)\n",
        "y_scaled = scaler_y.fit_transform(y_raw)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    outputs = tf.keras.layers.Dense(3)(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3. Переднавчання\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "# 4. Physics-Informed навчання\n",
        "lambda_phy = 0.1\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "epochs = 300\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.permutation(len(X_train))\n",
        "    X_train, y_train = X_train[idx], y_train[idx]\n",
        "    total_loss_epoch = []\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = tf.convert_to_tensor(X_train[i:i+batch_size])\n",
        "        y_batch = tf.convert_to_tensor(y_train[i:i+batch_size])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            mse_loss = tf.reduce_mean(tf.square(y_batch - y_pred))\n",
        "\n",
        "            y_pred_np = y_pred.numpy()\n",
        "            y_pred_phys = scaler_y.inverse_transform(y_pred_np)\n",
        "            strain = y_pred_phys[:, 0]\n",
        "            stress = y_pred_phys[:, 1]\n",
        "\n",
        "            X_inv = scaler_X.inverse_transform(X_batch.numpy())\n",
        "            E = X_inv[:, 2]\n",
        "\n",
        "            stress_pred = E * strain\n",
        "            physics_loss = np.mean(((stress - stress_pred) / (stress + 1e-6)) ** 2)\n",
        "\n",
        "            total_loss = mse_loss + lambda_phy * physics_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        total_loss_epoch.append(total_loss.numpy())\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse_loss.numpy():.6f}, Physics: {physics_loss:.6f}, Total: {np.mean(total_loss_epoch):.6f}\")\n",
        "\n",
        "# 5. Збереження моделі\n",
        "model.save(\"pinn_deformation_model.keras\")\n",
        "\n",
        "# 6. Тестовий приклад\n",
        "example = np.array([[1.5, 5.0, 210000]])\n",
        "example_scaled = scaler_X.transform(example)\n",
        "prediction_scaled = model.predict(example_scaled)\n",
        "prediction = scaler_y.inverse_transform(prediction_scaled)\n",
        "\n",
        "print(\"\\n🧪 Перевірка:\")\n",
        "print(f\"Вхід: товщина = 1.5 мм, зміщення = 5.0 мм, E = 210000 МПа\")\n",
        "print(f\"→ Прогнозована деформація (MaxPlasticStrain): {prediction[0,0]:.6f}\")\n",
        "print(f\"→ Прогнозоване напруження (MaxVonMises):     {prediction[0,1]:.2f} МПа\")\n",
        "print(f\"→ Прогнозоване зміщення (MaxDisp):           {prediction[0,2]:.4f} мм\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtE69pNKmWYY",
        "outputId": "b12c0fd8-c045-4d9c-d034-1490b41ee3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.107652, Physics: 1.000000, Total: 0.507674\n",
            "Epoch 50, MSE: 0.202383, Physics: 1.000000, Total: 0.460537\n",
            "Epoch 100, MSE: 0.098756, Physics: 1.000000, Total: 0.426115\n",
            "Epoch 150, MSE: 0.033200, Physics: 1.000000, Total: 0.386099\n",
            "Epoch 200, MSE: 0.075806, Physics: 1.000000, Total: 0.349420\n",
            "Epoch 250, MSE: 0.054027, Physics: 1.000000, Total: 0.307461\n",
            "Epoch 299, MSE: 0.060569, Physics: 1.000000, Total: 0.273120\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "🧪 Перевірка:\n",
            "Вхід: товщина = 1.5 мм, зміщення = 5.0 мм, E = 210000 МПа\n",
            "→ Прогнозована деформація (MaxPlasticStrain): 0.048560\n",
            "→ Прогнозоване напруження (MaxVonMises):     212367632.00 МПа\n",
            "→ Прогнозоване зміщення (MaxDisp):           2347.8931 мм\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example1 = np.array([[0.55, 18.6, 71.5]])\n",
        "example_scaled1 = scaler_X.transform(example1)\n",
        "prediction_scaled1 = model.predict(example_scaled1)\n",
        "prediction1 = scaler_y.inverse_transform(prediction_scaled1)\n",
        "\n",
        "print(\"\\n🧪 Перевірка:\")\n",
        "print(f\"Вхід: товщина = 1.5 мм, зміщення = 5.0 мм, E = 210000 МПа\")\n",
        "print(f\"→ Прогнозована деформація (MaxPlasticStrain): {prediction1[0,0]:.6f}\")\n",
        "print(f\"→ Прогнозоване напруження (MaxVonMises):     {prediction1[0,1]:.6f} МПа\")\n",
        "print(f\"→ Прогнозоване зміщення (MaxDisp):           {prediction1[0,2]:.6f} мм\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdfZ4u3ZvO1b",
        "outputId": "764bf68c-c521-49a2-8f4a-5f432241e6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\n",
            "🧪 Перевірка:\n",
            "Вхід: товщина = 1.5 мм, зміщення = 5.0 мм, E = 210000 МПа\n",
            "→ Прогнозована деформація (MaxPlasticStrain): 0.015290\n",
            "→ Прогнозоване напруження (MaxVonMises):     200255216.000000 МПа\n",
            "→ Прогнозоване зміщення (MaxDisp):           -235.026428 мм\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Завантаження та підготовка даних\n",
        "data = pd.read_csv(\"Results (1).csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "data = data.dropna()\n",
        "\n",
        "# Перевіримо розподіл MaxDisp\n",
        "plt.hist(data[\"MaxDisp\"], bins=100)\n",
        "plt.title(\"Розподіл MaxDisp\")\n",
        "plt.show()\n",
        "\n",
        "X_raw = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values.astype(np.float32)\n",
        "y_raw = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']].values.astype(np.float32)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_raw)\n",
        "y_scaled = scaler_y.fit_transform(y_raw)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі з нормалізацією та регуляризацією\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.LayerNormalization()(inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    outputs = tf.keras.layers.Dense(3)(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3. Попереднє навчання зі зваженою MSE\n",
        "def weighted_mse(y_true, y_pred):\n",
        "    weights = tf.constant([1.0, 1.0, 10.0], dtype=tf.float32)\n",
        "    return tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=weighted_mse)\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "# 4. Physics-Informed навчання\n",
        "lambda_phy = 0.1\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "epochs = 300\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.permutation(len(X_train))\n",
        "    X_train, y_train = X_train[idx], y_train[idx]\n",
        "    total_loss_epoch = []\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = tf.convert_to_tensor(X_train[i:i+batch_size])\n",
        "        y_batch = tf.convert_to_tensor(y_train[i:i+batch_size])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            mse_loss = weighted_mse(y_batch, y_pred)\n",
        "\n",
        "            y_pred_np = y_pred.numpy()\n",
        "            y_pred_phys = scaler_y.inverse_transform(y_pred_np)\n",
        "            strain = y_pred_phys[:, 0]\n",
        "            stress = y_pred_phys[:, 1]\n",
        "\n",
        "            X_inv = scaler_X.inverse_transform(X_batch.numpy())\n",
        "            E = X_inv[:, 2]\n",
        "\n",
        "            stress_pred = E * strain\n",
        "            physics_loss = np.mean(((stress - stress_pred) / (stress + 1e-6)) ** 2)\n",
        "\n",
        "            total_loss = mse_loss + lambda_phy * physics_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        total_loss_epoch.append(total_loss.numpy())\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse_loss.numpy():.6f}, Physics: {physics_loss:.6f}, Total: {np.mean(total_loss_epoch):.6f}\")\n",
        "\n",
        "# 5. Збереження моделі\n",
        "model.save(\"pinn_deformation_model.keras\")\n",
        "\n",
        "# 6. Тестовий приклад\n",
        "example = np.array([[0.55, 18.6, 71.5]])  # приклад з датасету\n",
        "example_scaled = scaler_X.transform(example)\n",
        "prediction_scaled = model.predict(example_scaled)\n",
        "prediction = scaler_y.inverse_transform(prediction_scaled)\n",
        "\n",
        "print(\"\\n🧪 Перевірка (реальні значення):\")\n",
        "print(\"Очікувано:  0.015199 | 199349210.442306 | 0.020338\")\n",
        "print(\"Прогноз NN:\")\n",
        "print(f\"{prediction[0,0]:.6f}\")  # MaxPlasticStrain\n",
        "print(f\"{prediction[0,1]:.6f}\")  # MaxVonMises\n",
        "print(f\"{prediction[0,2]:.6f}\")  # MaxDisp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "9JL4bOUw8LEu",
        "outputId": "645703b8-4505-4435-d0a6-c0c3b7f1be5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGzCAYAAAAxPS2EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAN0tJREFUeJzt3Xl4VOX9//9XQsiQECZhS0IkQCgKhM2CFUYRUVICxAqKVZYqKqJAsAWUrVpAawtilQ9ugNISrk+LCP0UFKJgBAGRsBiNsqYCocHCBBAzw5oAub9/+Mv5MSYsYTG54fm4rnNd5Nzvc8773E2cV8+cMxNkjDECAACwSHBFNwAAAFBeBBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAVqnPnzurcuXNFt1FpBAUFaeLEiRXdBlDpEWCASiItLU1BQUHOUq1aNd1www0aNmyY8vPzK7o9a6xcudKZw7///e9l1tx6660KCgpSy5Ytr2gvjRo1cnoJDg5WVFSUWrVqpccff1zr16+/oscGrnYhFd0AgEDPP/+8EhISdOLECa1Zs0bTp0/XBx98oM2bNys8PLyi27vsPvrooyuy32rVqmnu3Ln6zW9+E7B+9+7dWrt2rapVq3ZFjvtjN954o5566ilJ0uHDh7Vt2zYtWLBAb7/9tkaMGKFXXnkloP748eMKCeE/zcD58FcCVDLdu3fXTTfdJEl67LHHVLt2bb3yyit677331Ldv3wru7vILDQ29Ivvt0aOH3n//fR08eFB16tRx1s+dO1cxMTG6/vrr9f3331+RY5/puuuuKxWiXnzxRfXr109Tp07V9ddfryFDhjhjP1WwAmzHW0hAJXfnnXdKknJzc511u3bt0q9//WvVqlVL4eHh6tChg9LT0wO2W7NmjTp27Kg6deqoWrVqaty4scaMGaMTJ044NT9+2+rHy4/vxfjyyy/VvXt3ud1uRUREqEuXLlq3bl2ZfT/88MNl7vPhhx8OqDvbPTBn2/5C7w/p2bOnXC6XFixYELB+7ty5uv/++1WlSpVS28yePVt33nmnoqOj5XK5lJiYqOnTpwfUrFixQsHBwRo/fnyp/QYFBZWqL0tYWJj+93//V7Vq1dKf/vQnGWOcsR+f4+HDhzV8+HA1atRILpdL0dHR+uUvf6kvvvjCqencubNatmyprKws3XLLLQoLC1NCQoJmzJhx3l4AW3EFBqjkdu7cKUmqXbu2JCk/P1+33HKLjh07pt/+9reqXbu25syZo7vvvlv//Oc/dc8990j64YWvefPmuv/++xUeHq7MzExNmTJFx44d02uvvRZwjJK3rUocOXIk4KqAJG3ZskW33Xab3G63Ro8erapVq2rmzJnq3LmzVq1apfbt25fq3eVyadasWc7Pjz32WLnOvU6dOpo6darz84MPPnjB24aHh6tnz5565513nHP56quvtGXLFs2aNUtff/11qW2mT5+uFi1a6O6771ZISIgWL16soUOHqri4WKmpqZJ+CJRDhw7VpEmT1KtXL7Vt21b79u3Tk08+qaSkJA0ePPiC+ouIiNA999yjv/71r9q6datatGhRZt3gwYP1z3/+U8OGDVNiYqK+++47rVmzRtu2bVPbtm2duu+//149evTQ/fffr759+2r+/PkaMmSIQkND9eijj17wvAHWMAAqhdmzZxtJ5uOPPzYHDhwwe/bsMfPmzTO1a9c2YWFh5ttvvzXGGDN8+HAjyXz66afOtocPHzYJCQmmUaNG5vTp02c9Ro8ePUzLli1LHXPjxo0BdQcOHDCSzIQJE5x1vXr1MqGhoWbnzp3Our1795oaNWqYTp06lTpWv379TERERMC66tWrmwEDBgSsu/32283tt99eavv+/fubhISEgHU/7qksn3zyiZFkFixYYJYsWWKCgoJMXl6eMcaYUaNGmcaNGzvHbdGiRcC2x44dK7W/5ORkZ5sSR48eNU2aNDEtWrQwJ06cMCkpKcbtdpv//Oc/AXUNGzY0KSkpZ+116tSpRpJ57733znqOkZGRJjU19ZznfPvttxtJ5uWXX3bWFRYWmhtvvNFER0eboqKic24P2Ii3kIBKJikpSXXr1lV8fLz69OmjiIgILVy4UNddd50k6YMPPtDNN9+sjh07OttERETo8ccf1+7du7V169aA/R06dEj79u3TokWLlJmZqU6dOpW7p9OnT+ujjz5Sr1691LhxY2d9vXr11K9fP61Zs0Z+vz9gmxMnTlzS/RxFRUVyuVwXvb0kde3aVbVq1dK8efNkjNG8efPOeR9RWFiY82+fz6eDBw/q9ttv165du+Tz+Zyx8PBwpaWladu2berUqZPS09M1depUNWjQoFz9RURESPrhatnZREVFaf369dq7d+859xUSEqInnnjC+Tk0NFRPPPGE9u/fr6ysrHL1BdiAAANUMm+88YYyMjL0ySefaOvWrdq1a5eSk5Od8f/85z9q2rRpqe2aN2/ujJ8pMTFRcXFxuueee9SzZ09Nmzat3D0dOHBAx44dO+txi4uLtWfPnoD1Bw8eVGRkZLmPVaKgoMB5gb9YVatW1a9//WvNnTtXq1ev1p49e9SvX7+z1n/22WdKSkpS9erVFRUVpbp16+r3v/+9JAUEGOmHR7GHDBmiDRs2KDk5+aLepjly5IgkqUaNGmetmTJlijZv3qz4+HjdfPPNmjhxonbt2lWqLi4uTtWrVw9Yd8MNN0j64ckr4GpDgAEqmZtvvllJSUnq3LmzmjdvruDgS/szXbBggZYsWaLnnntO77777k/2IWm7d+9Wo0aNLnp7r9er2NjYS+6jX79+ys7O1sSJE9WmTRslJiaWWbdz50516dJFBw8e1CuvvKL09HRlZGRoxIgRkqTi4uKA+sLCQq1cudLZ9tixY+XubfPmzZKkJk2anLXm/vvv165du/Taa68pLi5OL730klq0aKEPP/yw3McDriYEGMAyDRs2VE5OTqn127dvd8bPdNtttyklJUXjx4/XuHHjNGnSJOf/+V+ounXrKjw8/KzHDQ4OVnx8vLPuwIEDysvLcx4HL6+TJ09qx44dzlWlS9GxY0c1aNBAK1euPOfVl8WLF6uwsFDvv/++nnjiCfXo0UNJSUkBbyudacKECdq2bZv+8pe/KDc3V2PHji1XX0eOHNHChQsVHx9/3vOsV6+ehg4dqkWLFik3N1e1a9fWn/70p4CavXv36ujRowHr/v3vf0vSJQVJoLIiwACW6dGjhzZs2KDMzExn3dGjR/XWW2+pUaNGZ73CIP3wtk5xcbFOnjxZrmNWqVJFXbt21XvvvRfwdkR+fr7mzp2rjh07yu12O+tLHl3u2bNnuY5T4r333tPx48edR8gvRVBQkF599VVNmDDhnE8xlTxWbc54pNnn82n27NmlatevX6+//OUvGj58uJ566imNGjVKr7/+ulatWnVBPR0/flwPPvigDh06pGeeeUZBQUFl1p0+fbrUW1fR0dGKi4tTYWFhwPpTp05p5syZzs9FRUWaOXOm6tatq3bt2l1QX4BNeIwasMzYsWP1zjvvqHv37vrtb3+rWrVqac6cOcrNzdX//d//OW85DR06VFWrVlXTpk0VHBysNWvWaO7cubrrrrtUs2bNch/3hRdeUEZGhjp27KihQ4cqJCREM2fOVGFhoaZMmeLUvfHGG3r22WdVt25d7dy503kMXPrhRXbXrl3KyMjQL3/5y1LHOHbsmCZMmKA333xTt9xyi7p27XoRM1Raz549zxumunbtqtDQUP3qV7/SE088oSNHjujtt99WdHS09u3b59SdOHFCAwYM0PXXX+9cBXnuuee0ePFiPfLII9q0aVPAvSj//e9/na80OHLkiLZu3aoFCxbI6/XqqaeeCrjx9scOHz6s+vXr67777lObNm0UERGhjz/+WBs3btTLL78cUBsXF6cXX3xRu3fv1g033KB3331X2dnZeuutt1S1atVyzxlQ6VX0Y1AAfnC2R5rLsnPnTnPfffeZqKgoU61aNXPzzTebJUuWBNRMnz7dtGrVylSvXt1ERESYxMRE89xzz5kjR46c95hlPUZtjDFffPGFSU5ONhERESY8PNzccccdZu3atQE1ks67nPnY9JmPUX/77bcmPj7eDB8+3Ph8vlLnXVZPP3bmY9TnUtZj1O+//75p3bq1qVatmmnUqJF58cUXzd/+9jcjyeTm5hpjjBkxYoSpUqWKWb9+fcC2n3/+uQkJCTFDhgxx1jVs2NA556CgION2u02LFi3MoEGDSm1f1jkWFhaaUaNGmTZt2pgaNWqY6tWrmzZt2pg333yzzHP5/PPPjcfjMdWqVTMNGzY0r7/++jnnALBZkDFnXC8FgEsUFBSkTz755KzfMJ2Wlqa0tDTnBlhcus6dO+vgwYPOTcHAtYB7YAAAgHUIMAAuq/79+ysmJuas4z/72c/KvP8FAMqDt5AAwHK8hYRrEQEGAABYh7eQAACAdQgwAADAOuX+ILv//ve/GjNmjD788EMdO3ZMTZo00ezZs52PDDfGaMKECXr77bdVUFCgW2+9VdOnT9f111/v7OPQoUN68skntXjxYgUHB6t3796aNm1awBe3ff3110pNTdXGjRtVt25dPfnkkxo9evQF91lcXKy9e/eqRo0aZ/2USwAAULkYY3T48GHFxcWd+7vgyvOhMYcOHTINGzY0Dz/8sFm/fr3ZtWuXWbZsmdmxY4dTM3nyZBMZGWkWLVpkvvrqK3P33XebhIQEc/z4caemW7dupk2bNmbdunXm008/NU2aNDF9+/Z1xn0+n4mJiTH9+/c3mzdvNu+8844JCwszM2fOvOBe9+zZc0EfqMXCwsLCwsJS+ZY9e/ac83W+XDfxjh07Vp999pk+/fTTMseNMYqLi9NTTz2lp59+WtIP3yUSExOjtLQ09enTR9u2bVNiYqI2btzoXLVZunSpevTooW+//VZxcXGaPn26nnnmGXm9XoWGhjrHXrRokfOFdefj8/kUFRWlPXv2BHxHCwAAqLz8fr/i4+NVUFCgyMjIs9aV6y2k999/X8nJyfr1r3+tVatW6brrrtPQoUM1aNAgSVJubq68Xq+SkpKcbSIjI9W+fXtlZmaqT58+yszMVFRUVMC31CYlJSk4OFjr16/XPffco8zMTHXq1MkJL5KUnJysF198Ud9//32Z3+NSWFgY8OVmhw8fliS53W4CDAAAljnf7R/luol3165dzv0sy5Yt05AhQ/Tb3/5Wc+bMkSR5vV5JKvUhVjExMc6Y1+tVdHR0wHhISIhq1aoVUFPWPs48xo9NmjRJkZGRzhIfH1+eUwMAABYpV4ApLi5W27Zt9ec//1k///nP9fjjj2vQoEGaMWPGlervgo0bN04+n89Z9uzZU9EtAQCAK6RcAaZevXpKTEwMWNe8eXPl5eVJkmJjYyVJ+fn5ATX5+fnOWGxsrPbv3x8wfurUKR06dCigpqx9nHmMH3O5XM7bRbxtBADA1a1cAebWW29VTk5OwLp///vfatiwoSQpISFBsbGxWr58uTPu9/u1fv16eTweSZLH41FBQYGysrKcmhUrVqi4uFjt27d3alavXq2TJ086NRkZGWratGmZ978AAIBrS7kCzIgRI7Ru3Tr9+c9/1o4dOzR37ly99dZbSk1NlfTDDTfDhw/XCy+8oPfff1+bNm3SQw89pLi4OPXq1UvSD1dsunXrpkGDBmnDhg367LPPNGzYMPXp00dxcXGSpH79+ik0NFQDBw7Uli1b9O6772ratGkaOXLk5T17AABgpwv+YJX/z+LFi03Lli2Ny+UyzZo1M2+99VbAeHFxsfnDH/5gYmJijMvlMl26dDE5OTkBNd99953p27eviYiIMG632zzyyCPm8OHDATVfffWV6dixo3G5XOa6664zkydPLlefPp/PSDI+n6+8pwgAACrIhb5+X7Vf5uj3+xUZGSmfz8f9MAAAWOJCX7/5LiQAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHVCKroBGzUam15q3e7JKRXQCQAA1yauwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1yhVgJk6cqKCgoIClWbNmzviJEyeUmpqq2rVrKyIiQr1791Z+fn7APvLy8pSSkqLw8HBFR0dr1KhROnXqVEDNypUr1bZtW7lcLjVp0kRpaWkXf4YAAOCqU+4rMC1atNC+ffucZc2aNc7YiBEjtHjxYi1YsECrVq3S3r17de+99zrjp0+fVkpKioqKirR27VrNmTNHaWlpGj9+vFOTm5urlJQU3XHHHcrOztbw4cP12GOPadmyZZd4qgAA4GoRUu4NQkIUGxtbar3P59Nf//pXzZ07V3feeackafbs2WrevLnWrVunDh066KOPPtLWrVv18ccfKyYmRjfeeKP++Mc/asyYMZo4caJCQ0M1Y8YMJSQk6OWXX5YkNW/eXGvWrNHUqVOVnJx8iacLAACuBuW+AvPNN98oLi5OjRs3Vv/+/ZWXlydJysrK0smTJ5WUlOTUNmvWTA0aNFBmZqYkKTMzU61atVJMTIxTk5ycLL/fry1btjg1Z+6jpKZkH2dTWFgov98fsAAAgKtTuQJM+/btlZaWpqVLl2r69OnKzc3VbbfdpsOHD8vr9So0NFRRUVEB28TExMjr9UqSvF5vQHgpGS8ZO1eN3+/X8ePHz9rbpEmTFBkZ6Szx8fHlOTUAAGCRcr2F1L17d+ffrVu3Vvv27dWwYUPNnz9fYWFhl7258hg3bpxGjhzp/Oz3+wkxAABcpS7pMeqoqCjdcMMN2rFjh2JjY1VUVKSCgoKAmvz8fOeemdjY2FJPJZX8fL4at9t9zpDkcrnkdrsDFgAAcHW6pABz5MgR7dy5U/Xq1VO7du1UtWpVLV++3BnPyclRXl6ePB6PJMnj8WjTpk3av3+/U5ORkSG3263ExESn5sx9lNSU7AMAAKBcAebpp5/WqlWrtHv3bq1du1b33HOPqlSpor59+yoyMlIDBw7UyJEj9cknnygrK0uPPPKIPB6POnToIEnq2rWrEhMT9eCDD+qrr77SsmXL9Oyzzyo1NVUul0uSNHjwYO3atUujR4/W9u3b9eabb2r+/PkaMWLE5T97AABgpXLdA/Ptt9+qb9+++u6771S3bl117NhR69atU926dSVJU6dOVXBwsHr37q3CwkIlJyfrzTffdLavUqWKlixZoiFDhsjj8ah69eoaMGCAnn/+eacmISFB6enpGjFihKZNm6b69etr1qxZPEINAAAcQcYYU9FNXAl+v1+RkZHy+XyX/X6YRmPTS63bPTnlsh4DAIBr0YW+fvNdSAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYJ1LCjCTJ09WUFCQhg8f7qw7ceKEUlNTVbt2bUVERKh3797Kz88P2C4vL08pKSkKDw9XdHS0Ro0apVOnTgXUrFy5Um3btpXL5VKTJk2UlpZ2Ka0CAICryEUHmI0bN2rmzJlq3bp1wPoRI0Zo8eLFWrBggVatWqW9e/fq3nvvdcZPnz6tlJQUFRUVae3atZozZ47S0tI0fvx4pyY3N1cpKSm64447lJ2dreHDh+uxxx7TsmXLLrZdAABwFbmoAHPkyBH1799fb7/9tmrWrOms9/l8+utf/6pXXnlFd955p9q1a6fZs2dr7dq1WrdunSTpo48+0tatW/X3v/9dN954o7p3764//vGPeuONN1RUVCRJmjFjhhISEvTyyy+refPmGjZsmO677z5NnTr1MpwyAACw3UUFmNTUVKWkpCgpKSlgfVZWlk6ePBmwvlmzZmrQoIEyMzMlSZmZmWrVqpViYmKcmuTkZPn9fm3ZssWp+fG+k5OTnX2UpbCwUH6/P2ABAABXp5DybjBv3jx98cUX2rhxY6kxr9er0NBQRUVFBayPiYmR1+t1as4MLyXjJWPnqvH7/Tp+/LjCwsJKHXvSpEl67rnnyns6AADAQuW6ArNnzx797ne/0z/+8Q9Vq1btSvV0UcaNGyefz+cse/bsqeiWAADAFVKuAJOVlaX9+/erbdu2CgkJUUhIiFatWqVXX31VISEhiomJUVFRkQoKCgK2y8/PV2xsrCQpNja21FNJJT+fr8btdpd59UWSXC6X3G53wAIAAK5O5QowXbp00aZNm5Sdne0sN910k/r37+/8u2rVqlq+fLmzTU5OjvLy8uTxeCRJHo9HmzZt0v79+52ajIwMud1uJSYmOjVn7qOkpmQfAADg2laue2Bq1Kihli1bBqyrXr26ateu7awfOHCgRo4cqVq1asntduvJJ5+Ux+NRhw4dJEldu3ZVYmKiHnzwQU2ZMkVer1fPPvusUlNT5XK5JEmDBw/W66+/rtGjR+vRRx/VihUrNH/+fKWnp1+OcwYAAJYr90285zN16lQFBwerd+/eKiwsVHJyst58801nvEqVKlqyZImGDBkij8ej6tWra8CAAXr++eedmoSEBKWnp2vEiBGaNm2a6tevr1mzZik5OflytwsAACwUZIwxFd3EleD3+xUZGSmfz3fZ74dpNLb0laDdk1Mu6zEAALgWXejrN9+FBAAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1ilXgJk+fbpat24tt9stt9stj8ejDz/80Bk/ceKEUlNTVbt2bUVERKh3797Kz88P2EdeXp5SUlIUHh6u6OhojRo1SqdOnQqoWblypdq2bSuXy6UmTZooLS3t4s8QAABcdcoVYOrXr6/JkycrKytLn3/+ue6880717NlTW7ZskSSNGDFCixcv1oIFC7Rq1Srt3btX9957r7P96dOnlZKSoqKiIq1du1Zz5sxRWlqaxo8f79Tk5uYqJSVFd9xxh7KzszV8+HA99thjWrZs2WU6ZQAAYLsgY4y5lB3UqlVLL730ku677z7VrVtXc+fO1X333SdJ2r59u5o3b67MzEx16NBBH374oe666y7t3btXMTExkqQZM2ZozJgxOnDggEJDQzVmzBilp6dr8+bNzjH69OmjgoICLV269IL78vv9ioyMlM/nk9vtvpRTLKXR2PRS63ZPTrmsxwAA4Fp0oa/fF30PzOnTpzVv3jwdPXpUHo9HWVlZOnnypJKSkpyaZs2aqUGDBsrMzJQkZWZmqlWrVk54kaTk5GT5/X7nKk5mZmbAPkpqSvZxNoWFhfL7/QELAAC4OpU7wGzatEkRERFyuVwaPHiwFi5cqMTERHm9XoWGhioqKiqgPiYmRl6vV5Lk9XoDwkvJeMnYuWr8fr+OHz9+1r4mTZqkyMhIZ4mPjy/vqQEAAEuUO8A0bdpU2dnZWr9+vYYMGaIBAwZo69atV6K3chk3bpx8Pp+z7Nmzp6JbAgAAV0hIeTcIDQ1VkyZNJEnt2rXTxo0bNW3aND3wwAMqKipSQUFBwFWY/Px8xcbGSpJiY2O1YcOGgP2VPKV0Zs2Pn1zKz8+X2+1WWFjYWftyuVxyuVzlPR0AAGChS/4cmOLiYhUWFqpdu3aqWrWqli9f7ozl5OQoLy9PHo9HkuTxeLRp0ybt37/fqcnIyJDb7VZiYqJTc+Y+SmpK9gEAAFCuKzDjxo1T9+7d1aBBAx0+fFhz587VypUrtWzZMkVGRmrgwIEaOXKkatWqJbfbrSeffFIej0cdOnSQJHXt2lWJiYl68MEHNWXKFHm9Xj377LNKTU11rp4MHjxYr7/+ukaPHq1HH31UK1as0Pz585WeXvrJHwAAcG0qV4DZv3+/HnroIe3bt0+RkZFq3bq1li1bpl/+8peSpKlTpyo4OFi9e/dWYWGhkpOT9eabbzrbV6lSRUuWLNGQIUPk8XhUvXp1DRgwQM8//7xTk5CQoPT0dI0YMULTpk1T/fr1NWvWLCUnJ1+mUwYAALa75M+Bqaz4HBgAAOxzxT8HBgAAoKIQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHXKFWAmTZqkX/ziF6pRo4aio6PVq1cv5eTkBNScOHFCqampql27tiIiItS7d2/l5+cH1OTl5SklJUXh4eGKjo7WqFGjdOrUqYCalStXqm3btnK5XGrSpInS0tIu7gwBAMBVp1wBZtWqVUpNTdW6deuUkZGhkydPqmvXrjp69KhTM2LECC1evFgLFizQqlWrtHfvXt17773O+OnTp5WSkqKioiKtXbtWc+bMUVpamsaPH+/U5ObmKiUlRXfccYeys7M1fPhwPfbYY1q2bNllOGUAAGC7IGOMudiNDxw4oOjoaK1atUqdOnWSz+dT3bp1NXfuXN13332SpO3bt6t58+bKzMxUhw4d9OGHH+quu+7S3r17FRMTI0maMWOGxowZowMHDig0NFRjxoxRenq6Nm/e7ByrT58+Kigo0NKlSy+oN7/fr8jISPl8Prnd7os9xTI1Gpteat3uySmX9RgAAFyLLvT1+5LugfH5fJKkWrVqSZKysrJ08uRJJSUlOTXNmjVTgwYNlJmZKUnKzMxUq1atnPAiScnJyfL7/dqyZYtTc+Y+SmpK9lGWwsJC+f3+gAUAAFydLjrAFBcXa/jw4br11lvVsmVLSZLX61VoaKiioqICamNiYuT1ep2aM8NLyXjJ2Llq/H6/jh8/XmY/kyZNUmRkpLPEx8df7KkBAIBK7qIDTGpqqjZv3qx58+Zdzn4u2rhx4+Tz+Zxlz549Fd0SAAC4QkIuZqNhw4ZpyZIlWr16terXr++sj42NVVFRkQoKCgKuwuTn5ys2Ntap2bBhQ8D+Sp5SOrPmx08u5efny+12KywsrMyeXC6XXC7XxZwOAACwTLmuwBhjNGzYMC1cuFArVqxQQkJCwHi7du1UtWpVLV++3FmXk5OjvLw8eTweSZLH49GmTZu0f/9+pyYjI0Nut1uJiYlOzZn7KKkp2QcAALi2lesKTGpqqubOnav33ntPNWrUcO5ZiYyMVFhYmCIjIzVw4ECNHDlStWrVktvt1pNPPimPx6MOHTpIkrp27arExEQ9+OCDmjJlirxer5599lmlpqY6V1AGDx6s119/XaNHj9ajjz6qFStWaP78+UpPL/30DwAAuPaU6wrM9OnT5fP51LlzZ9WrV89Z3n33Xadm6tSpuuuuu9S7d2916tRJsbGx+te//uWMV6lSRUuWLFGVKlXk8Xj0m9/8Rg899JCef/55pyYhIUHp6enKyMhQmzZt9PLLL2vWrFlKTk6+DKcMAABsd0mfA1OZ8TkwAADY5yf5HBgAAICKQIABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWKXeAWb16tX71q18pLi5OQUFBWrRoUcC4MUbjx49XvXr1FBYWpqSkJH3zzTcBNYcOHVL//v3ldrsVFRWlgQMH6siRIwE1X3/9tW677TZVq1ZN8fHxmjJlSvnPDgAAXJXKHWCOHj2qNm3a6I033ihzfMqUKXr11Vc1Y8YMrV+/XtWrV1dycrJOnDjh1PTv319btmxRRkaGlixZotWrV+vxxx93xv1+v7p27aqGDRsqKytLL730kiZOnKi33nrrIk4RAABcbYKMMeaiNw4K0sKFC9WrVy9JP1x9iYuL01NPPaWnn35akuTz+RQTE6O0tDT16dNH27ZtU2JiojZu3KibbrpJkrR06VL16NFD3377reLi4jR9+nQ988wz8nq9Cg0NlSSNHTtWixYt0vbt2y+oN7/fr8jISPl8Prnd7os9xTI1Gpteat3uySmX9RgAAFyLLvT1+7LeA5Obmyuv16ukpCRnXWRkpNq3b6/MzExJUmZmpqKiopzwIklJSUkKDg7W+vXrnZpOnTo54UWSkpOTlZOTo++//77MYxcWFsrv9wcsAADg6nRZA4zX65UkxcTEBKyPiYlxxrxer6KjowPGQ0JCVKtWrYCasvZx5jF+bNKkSYqMjHSW+Pj4Sz8hAABQKV01TyGNGzdOPp/PWfbs2VPRLQEAgCvksgaY2NhYSVJ+fn7A+vz8fGcsNjZW+/fvDxg/deqUDh06FFBT1j7OPMaPuVwuud3ugAUAAFydLmuASUhIUGxsrJYvX+6s8/v9Wr9+vTwejyTJ4/GooKBAWVlZTs2KFStUXFys9u3bOzWrV6/WyZMnnZqMjAw1bdpUNWvWvJwtAwAAC5U7wBw5ckTZ2dnKzs6W9MONu9nZ2crLy1NQUJCGDx+uF154Qe+//742bdqkhx56SHFxcc6TSs2bN1e3bt00aNAgbdiwQZ999pmGDRumPn36KC4uTpLUr18/hYaGauDAgdqyZYveffddTZs2TSNHjrxsJw4AAOwVUt4NPv/8c91xxx3OzyWhYsCAAUpLS9Po0aN19OhRPf744yooKFDHjh21dOlSVatWzdnmH//4h4YNG6YuXbooODhYvXv31quvvuqMR0ZG6qOPPlJqaqratWunOnXqaPz48QGfFQMAAK5dl/Q5MJUZnwMDAIB9KuRzYAAAAH4KBBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGCdSh1g3njjDTVq1EjVqlVT+/bttWHDhopuCQAAVAKVNsC8++67GjlypCZMmKAvvvhCbdq0UXJysvbv31/RrQEAgApWaQPMK6+8okGDBumRRx5RYmKiZsyYofDwcP3tb3+r6NYAAEAFC6noBspSVFSkrKwsjRs3zlkXHByspKQkZWZmlrlNYWGhCgsLnZ99Pp8kye/3X/b+iguPlVp3JY4DAMC1puT11BhzzrpKGWAOHjyo06dPKyYmJmB9TEyMtm/fXuY2kyZN0nPPPVdqfXx8/BXp8cci/+cnOQwAANeEw4cPKzIy8qzjlTLAXIxx48Zp5MiRzs/FxcU6dOiQateuraCgoMt2HL/fr/j4eO3Zs0dut/uy7fdqwhydH3N0fszRuTE/58ccnV9lnCNjjA4fPqy4uLhz1lXKAFOnTh1VqVJF+fn5Aevz8/MVGxtb5jYul0sulytgXVRU1JVqUW63u9L8j11ZMUfnxxydH3N0bszP+TFH51fZ5uhcV15KVMqbeENDQ9WuXTstX77cWVdcXKzly5fL4/FUYGcAAKAyqJRXYCRp5MiRGjBggG666SbdfPPN+p//+R8dPXpUjzzySEW3BgAAKlilDTAPPPCADhw4oPHjx8vr9erGG2/U0qVLS93Y+1NzuVyaMGFCqber8P9jjs6POTo/5ujcmJ/zY47Oz+Y5CjLne04JAACgkqmU98AAAACcCwEGAABYhwADAACsQ4ABAADWIcAAAADrEGDK6Y033lCjRo1UrVo1tW/fXhs2bKjoli7ZxIkTFRQUFLA0a9bMGT9x4oRSU1NVu3ZtRUREqHfv3qU+JTkvL08pKSkKDw9XdHS0Ro0apVOnTgXUrFy5Um3btpXL5VKTJk2UlpZWqpfKMr+rV6/Wr371K8XFxSkoKEiLFi0KGDfGaPz48apXr57CwsKUlJSkb775JqDm0KFD6t+/v9xut6KiojRw4EAdOXIkoObrr7/WbbfdpmrVqik+Pl5Tpkwp1cuCBQvUrFkzVatWTa1atdIHH3xQ7l6uhPPN0cMPP1zq96pbt24BNVfzHE2aNEm/+MUvVKNGDUVHR6tXr17KyckJqKlMf1sX0svldiFz1Llz51K/R4MHDw6ouZrnaPr06WrdurXzSbkej0cffvhhuXq6aufH4ILNmzfPhIaGmr/97W9my5YtZtCgQSYqKsrk5+dXdGuXZMKECaZFixZm3759znLgwAFnfPDgwSY+Pt4sX77cfP7556ZDhw7mlltuccZPnTplWrZsaZKSksyXX35pPvjgA1OnTh0zbtw4p2bXrl0mPDzcjBw50mzdutW89tprpkqVKmbp0qVOTWWa3w8++MA888wz5l//+peRZBYuXBgwPnnyZBMZGWkWLVpkvvrqK3P33XebhIQEc/z4caemW7dupk2bNmbdunXm008/NU2aNDF9+/Z1xn0+n4mJiTH9+/c3mzdvNu+8844JCwszM2fOdGo+++wzU6VKFTNlyhSzdetW8+yzz5qqVauaTZs2lauXK+F8czRgwADTrVu3gN+rQ4cOBdRczXOUnJxsZs+ebTZv3myys7NNjx49TIMGDcyRI0ecmsr0t3W+Xipqjm6//XYzaNCggN8jn893zczR+++/b9LT082///1vk5OTY37/+9+bqlWrms2bN19QT1fz/BBgyuHmm282qampzs+nT582cXFxZtKkSRXY1aWbMGGCadOmTZljBQUFpmrVqmbBggXOum3bthlJJjMz0xjzwwtZcHCw8Xq9Ts306dON2+02hYWFxhhjRo8ebVq0aBGw7wceeMAkJyc7P1fW+f3xi3NxcbGJjY01L730krOuoKDAuFwu88477xhjjNm6dauRZDZu3OjUfPjhhyYoKMj897//NcYY8+abb5qaNWs6c2SMMWPGjDFNmzZ1fr7//vtNSkpKQD/t27c3TzzxxAX38lM4W4Dp2bPnWbe51uZo//79RpJZtWqV00Nl+du6kF5+Cj+eI2N+CDC/+93vzrrNtTZHxhhTs2ZNM2vWrGv+d4i3kC5QUVGRsrKylJSU5KwLDg5WUlKSMjMzK7Czy+Obb75RXFycGjdurP79+ysvL0+SlJWVpZMnTwacd7NmzdSgQQPnvDMzM9WqVauAT0lOTk6W3+/Xli1bnJoz91FSU7IPm+Y3NzdXXq83oNfIyEi1b98+YE6ioqJ00003OTVJSUkKDg7W+vXrnZpOnTopNDTUqUlOTlZOTo6+//57p+Zc83YhvVSklStXKjo6Wk2bNtWQIUP03XffOWPX2hz5fD5JUq1atSRVrr+tC+nlp/DjOSrxj3/8Q3Xq1FHLli01btw4HTt2zBm7lubo9OnTmjdvno4ePSqPx3PN/w5V2q8SqGwOHjyo06dPl/oqg5iYGG3fvr2Curo82rdvr7S0NDVt2lT79u3Tc889p9tuu02bN2+W1+tVaGhoqW/2jomJkdfrlSR5vd4y56Vk7Fw1fr9fx48f1/fff2/N/JacU1m9nnm+0dHRAeMhISGqVatWQE1CQkKpfZSM1axZ86zzduY+ztdLRenWrZvuvfdeJSQkaOfOnfr973+v7t27KzMzU1WqVLmm5qi4uFjDhw/XrbfeqpYtWzp9VZa/rQvp5Uora44kqV+/fmrYsKHi4uL09ddfa8yYMcrJydG//vUvp/erfY42bdokj8ejEydOKCIiQgsXLlRiYqKys7Ov6d8hAgzUvXt359+tW7dW+/bt1bBhQ82fP19hYWEV2Bls1qdPH+ffrVq1UuvWrfWzn/1MK1euVJcuXSqws59eamqqNm/erDVr1lR0K5XW2ebo8ccfd/7dqlUr1atXT126dNHOnTv1s5/97Kdus0I0bdpU2dnZ8vl8+uc//6kBAwZo1apVFd1WheMtpAtUp04dValSpdQd1fn5+YqNja2grq6MqKgo3XDDDdqxY4diY2NVVFSkgoKCgJozzzs2NrbMeSkZO1eN2+1WWFiYVfNb0s+5eo2NjdX+/fsDxk+dOqVDhw5dlnk7c/x8vVQWjRs3Vp06dbRjxw5J184cDRs2TEuWLNEnn3yi+vXrO+sr09/WhfRyJZ1tjsrSvn17SQr4Pbra5yg0NFRNmjRRu3btNGnSJLVp00bTpk275n+HCDAXKDQ0VO3atdPy5cuddcXFxVq+fLk8Hk8Fdnb5HTlyRDt37lS9evXUrl07Va1aNeC8c3JylJeX55y3x+PRpk2bAl6MMjIy5Ha7lZiY6NScuY+SmpJ92DS/CQkJio2NDejV7/dr/fr1AXNSUFCgrKwsp2bFihUqLi52/gPs8Xi0evVqnTx50qnJyMhQ06ZNVbNmTafmXPN2Ib1UFt9++62+++471atXT9LVP0fGGA0bNkwLFy7UihUrSr0VVpn+ti6klyvhfHNUluzsbEkK+D26mueoLMXFxSosLOR36IrcGnyVmjdvnnG5XCYtLc1s3brVPP744yYqKirg7m4bPfXUU2blypUmNzfXfPbZZyYpKcnUqVPH7N+/3xjzw6NxDRo0MCtWrDCff/658Xg8xuPxONuXPKbXtWtXk52dbZYuXWrq1q1b5mN6o0aNMtu2bTNvvPFGmY/pVZb5PXz4sPnyyy/Nl19+aSSZV155xXz55ZfmP//5jzHmh8dyo6KizHvvvWe+/vpr07NnzzIfo/75z39u1q9fb9asWWOuv/76gEeECwoKTExMjHnwwQfN5s2bzbx580x4eHipR4RDQkLMX/7yF7Nt2zYzYcKEMh8RPl8vP/UcHT582Dz99NMmMzPT5Obmmo8//ti0bdvWXH/99ebEiRPXxBwNGTLEREZGmpUrVwY8Anzs2DGnpjL9bZ2vl4qYox07dpjnn3/efP755yY3N9e89957pnHjxqZTp07XzByNHTvWrFq1yuTm5pqvv/7ajB071gQFBZmPPvrognq6mueHAFNOr732mmnQoIEJDQ01N998s1m3bl1Ft3TJHnjgAVOvXj0TGhpqrrvuOvPAAw+YHTt2OOPHjx83Q4cONTVr1jTh4eHmnnvuMfv27QvYx+7du0337t1NWFiYqVOnjnnqqafMyZMnA2o++eQTc+ONN5rQ0FDTuHFjM3v27FK9VJb5/eSTT4ykUsuAAQOMMT88mvuHP/zBxMTEGJfLZbp06WJycnIC9vHdd9+Zvn37moiICON2u80jjzxiDh8+HFDz1VdfmY4dOxqXy2Wuu+46M3ny5FK9zJ8/39xwww0mNDTUtGjRwqSnpweMX0gvV8K55ujYsWOma9eupm7duqZq1aqmYcOGZtCgQaXC6NU8R2XNjaSA3/vK9Ld1Ib1cbuebo7y8PNOpUydTq1Yt43K5TJMmTcyoUaMCPgfGmKt7jh599FHTsGFDExoaaurWrWu6dOnihJcL7elqnZ8gY4y5Mtd2AAAArgzugQEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdf4fPh1aTZaoryoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.475912, Physics: 1.000000, Total: 4.699279\n",
            "Epoch 50, MSE: 0.816220, Physics: 1.000000, Total: 4.659496\n",
            "Epoch 100, MSE: 0.720710, Physics: 1.000000, Total: 4.638310\n",
            "Epoch 150, MSE: 0.501261, Physics: 1.000000, Total: 4.610876\n",
            "Epoch 200, MSE: 0.372912, Physics: 1.000000, Total: 4.596482\n",
            "Epoch 250, MSE: 0.536421, Physics: 1.000000, Total: 4.567585\n",
            "Epoch 299, MSE: 0.470779, Physics: 1.000000, Total: 4.553410\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "🧪 Перевірка (реальні значення):\n",
            "Очікувано:  0.015199 | 199349210.442306 | 0.020338\n",
            "Прогноз NN:\n",
            "0.040052\n",
            "198257600.000000\n",
            "166.661026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Завантаження даних\n",
        "data = pd.read_csv(\"Results (1).csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "data = data.dropna()\n",
        "\n",
        "X_raw = data[['Thickness', 'Max_punch_displacement', 'Youngs_modulus']].values.astype(np.float32)\n",
        "y_raw = data[['MaxPlasticStrain', 'MaxVonMises', 'MaxDisp']].values.astype(np.float32)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X_raw)\n",
        "y_scaled = scaler_y.fit_transform(y_raw)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Побудова моделі\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(3,))\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(inputs)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='tanh')(x)\n",
        "    outputs = tf.keras.layers.Dense(3)(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3. Переднавчання (емпіричне)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "# 4. Physics-Informed навчання (з вагами для MSE)\n",
        "lambda_phy = 0.1\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "epochs = 300\n",
        "batch_size = 32\n",
        "\n",
        "# Ваги для кожного з виходів: [MaxPlasticStrain, MaxVonMises, MaxDisp]\n",
        "weights = tf.constant([5.0, 1.0, 10.0], dtype=tf.float32)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.permutation(len(X_train))\n",
        "    X_train, y_train = X_train[idx], y_train[idx]\n",
        "    total_loss_epoch = []\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = tf.convert_to_tensor(X_train[i:i+batch_size])\n",
        "        y_batch = tf.convert_to_tensor(y_train[i:i+batch_size])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "\n",
        "            # Зважений MSE\n",
        "            weighted_errors = tf.square(y_batch - y_pred) * weights\n",
        "            mse_loss = tf.reduce_mean(tf.reduce_sum(weighted_errors, axis=1))\n",
        "\n",
        "            # Physics loss\n",
        "            y_pred_np = y_pred.numpy()\n",
        "            y_pred_phys = scaler_y.inverse_transform(y_pred_np)\n",
        "            strain = y_pred_phys[:, 0]\n",
        "            stress = y_pred_phys[:, 1]\n",
        "\n",
        "            X_inv = scaler_X.inverse_transform(X_batch.numpy())\n",
        "            E = X_inv[:, 2]\n",
        "\n",
        "            stress_pred = E * strain\n",
        "            physics_loss = np.mean(((stress - stress_pred) / (stress + 1e-6)) ** 2)\n",
        "\n",
        "            total_loss = mse_loss + lambda_phy * physics_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        total_loss_epoch.append(total_loss.numpy())\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}, MSE: {mse_loss.numpy():.6f}, Physics: {physics_loss:.6f}, Total: {np.mean(total_loss_epoch):.6f}\")\n",
        "\n",
        "# 5. Збереження моделі\n",
        "model.save(\"pinn_deformation_model_fixed.keras\")\n",
        "\n",
        "# 6. Тестовий приклад\n",
        "example1 = np.array([[0.55, 18.6, 71.5]])\n",
        "example_scaled1 = scaler_X.transform(example1)\n",
        "prediction_scaled1 = model.predict(example_scaled1)\n",
        "prediction1 = scaler_y.inverse_transform(prediction_scaled1)\n",
        "\n",
        "print(\"\\n🧪 Перевірка (реальні значення):\")\n",
        "print(\"Очікувано:  0.015199 | 199349210.442306 | 0.020338\")\n",
        "print(\"Прогноз NN:\")\n",
        "print(f\"{prediction1[0,0]:.6f}\")\n",
        "print(f\"{prediction1[0,1]:.6f}\")\n",
        "print(f\"{prediction1[0,2]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3cxkzJj34b_",
        "outputId": "aa8c8c27-67c5-44e7-f7e8-74756e80a1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, MSE: 0.539952, Physics: 1.000000, Total: 11.085417\n",
            "Epoch 50, MSE: 0.217777, Physics: 1.000000, Total: 8.478932\n",
            "Epoch 100, MSE: 0.283364, Physics: 1.000000, Total: 7.347523\n",
            "Epoch 150, MSE: 0.139300, Physics: 1.000000, Total: 6.060500\n",
            "Epoch 200, MSE: 0.186701, Physics: 1.000000, Total: 4.789543\n",
            "Epoch 250, MSE: 0.445787, Physics: 1.000000, Total: 3.657020\n",
            "Epoch 299, MSE: 0.493829, Physics: 1.000000, Total: 2.716455\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\n",
            "🧪 Перевірка (реальні значення):\n",
            "Очікувано:  0.015199 | 199349210.442306 | 0.020338\n",
            "Прогноз NN:\n",
            "0.015258\n",
            "199096496.000000\n",
            "-27.170897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clear\n",
        "close all\n",
        "clc\n",
        "format long\n",
        "tic\n",
        "// Initialize COMSOL Model\n",
        "import com.comsol.model.*\n",
        "import com.comsol.model.util.*\n",
        "\n",
        "// Load the COMSOL Model\n",
        "model = mphopen('C:\\Users\\bpetr\\Desktop\\Metal_shape_forming.mph');\n",
        "\n",
        "// mphgetexpressions(model, 'param')\n",
        "\n",
        "%param_range=[1e-4 2e-2:2e-2:1.5];\n",
        "% Parameter settings\n",
        "\n",
        "N=14; % N^3 amount of points for NN\n",
        "\n",
        "% sigma0_values = linspace(190,212,23); % Range of sigma_0 (yield stress) values\n",
        "thickness_values=linspace(0.45,1.45,N);\n",
        "punch_max_values=linspace(18,28,N);\n",
        "You_m_values=linspace(75.5,175.5,N);\n",
        "% Pr_values=linspace(0.25,0.35,11);\n",
        "output_file = 'Results.csv'; % File to save the results\n",
        "\n",
        "\n",
        "\n",
        "fid = fopen(output_file, 'w');\n",
        "fprintf(fid, 'Thickness, Max_punch_displacement, Youngs_modulus, MaxPlasticStrain, MaxVonMises, MaxDisp, End_angle, Residual_angle\\n');\n",
        "\n",
        "% model.study('std1').create('param', 'Parametric');\n",
        "% model.study('std1').feature('param').set('pname', {'param'}); % Parameter name\n",
        "% model.study('std1').feature('param').set('plist', num2str(param_range))\n",
        "k=1;\n",
        "for thickness=thickness_values\n",
        "   for punch_max=punch_max_values\n",
        "       for i1=1:length(You_m_values)\n",
        "\n",
        "        \n",
        "   \n",
        "    th=num2str(thickness)+\"[mm]\";\n",
        "    You_m=You_m_values(i1);\n",
        "    model.param.set('th',th);\n",
        "    model.param.set('punch_max', num2str(punch_max));\n",
        "    model.param.set('You_m', num2str(You_m));\n",
        "    \n",
        "\n",
        "   % model.param.set('Pr', num2str(Pr));\n",
        "    % Run the simulation\n",
        "    try\n",
        "    model.study('std1').run();\n",
        "    \n",
        "    pd1 = mphplot(model,'pg1','rangenum',1);\n",
        "    my_plot1='my_plot1_';\n",
        "    idx=num2str(k);\n",
        "    C1={my_plot1,idx,'.png'};\n",
        "    saveas(gcf, strjoin(C1,''));\n",
        "    pd2 = mphplot(model,'pg2','rangenum',1);\n",
        "    my_plot2='my_plot2_';\n",
        "    C2={my_plot2,idx,'.png'};\n",
        "    saveas(gcf, strjoin(C2,''));\n",
        "    pd3 = mphplot(model,'pg3','rangenum',1);\n",
        "    my_plot3='my_plot3_';\n",
        "    C3={my_plot3,idx,'.png'};\n",
        "    saveas(gcf, strjoin(C3,''));\n",
        "    k=k+1;\n",
        "    close all\n",
        "    % Extract  the results\n",
        "  %  model.result.table.tags()\n",
        "    str = mphtable(model,'tbl3');\n",
        "    tbl_data = str.data;\n",
        "    min_angle=min(tbl_data(:,2));\n",
        "    end_angle=tbl_data(end,2);\n",
        "    residual_angle=end_angle-min_angle;\n",
        "\n",
        "     % model.result.tags()\n",
        "%     para_range=tbl_data(:,1);\n",
        "%     fig=mphplot(model,'pg3','createplot','off');\n",
        "    max_plastic_strain=mphmax(model, 'solid.epe','surface', 'dataset', 'dset1','solnum', 'end');\n",
        "    max_Mises_stress=mphmax(model, 'solid.mises','surface', 'dataset', 'dset1','solnum', 'end');\n",
        "    max_Displacement=mphmax(model, 'solid.disp','surface', 'dataset', 'dset1','solnum', 'end');\n",
        "    catch\n",
        "        continue\n",
        "    end\n",
        "    tableObj = model.result.table('tbl3');\n",
        "    tableObj.feature().clear();\n",
        "\n",
        "    % Write data to the file\n",
        "                                                                                                        \n",
        "    fprintf(fid, '%f, %f, %f, %f, %f, %f, %f, %f\\n', thickness,punch_max,You_m, max_plastic_strain, max_Mises_stress, max_Displacement,end_angle,residual_angle);\n",
        "    fprintf('Processed Thickness = %f mm, punch_max = %f, You_m = %f, max_plastic_strain = %f, max_Mises_stress = %f,max_Displacement = %f,End_angle = %f,Residual_angle = %f\\n',...\n",
        "        thickness,punch_max,You_m, max_plastic_strain, max_Mises_stress,max_Displacement,end_angle,residual_angle);\n",
        "\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n",
        "% save model\n",
        "mphsave(model,'C:\\Users\\bpetr\\Desktop\\\\Metal_shape_forming.mph')\n",
        "\n",
        "% Close the file\n",
        "fclose(fid);\n",
        "\n",
        "disp('Simulation complete. Results saved to Results.csv.');\n",
        "toc"
      ],
      "metadata": {
        "id": "DKk7JIt9YWdS"
      }
    }
  ]
}